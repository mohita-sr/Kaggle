{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "This notebook is a pure fork of the great notebook by @xhlulu : https://www.kaggle.com/xhlulu/disaster-nlp-keras-bert-using-tfhub (version 7)\n",
    "except training data (train.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7613, 5)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "train_orig = pd.read_csv(\"/kaggle/input/nlp-getting-started/train.csv\")\n",
    "test = pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\")\n",
    "submission = pd.read_csv(\"/kaggle/input/nlp-getting-started/sample_submission.csv\")\n",
    "train_orig.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While studying this model and my own models, I discovered that these kind of predictions are so sensitive to the training data. Next, I read tweets in training data and figure out, that some of them have errors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>229</th>\n",
       "      <td>328</td>\n",
       "      <td>annihilated</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Ready to get annihilated for the BUCS game</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301</th>\n",
       "      <td>443</td>\n",
       "      <td>apocalypse</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Short Reading\\n\\nApocalypse 21:1023 \\n\\nIn the...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>356</th>\n",
       "      <td>513</td>\n",
       "      <td>army</td>\n",
       "      <td>Studio</td>\n",
       "      <td>But if you build an army of 100 dogs and their...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1822</th>\n",
       "      <td>2619</td>\n",
       "      <td>crashed</td>\n",
       "      <td>NaN</td>\n",
       "      <td>My iPod crashed..... \\n#WeLoveYouLouis \\n#MTVH...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2536</th>\n",
       "      <td>3640</td>\n",
       "      <td>desolation</td>\n",
       "      <td>Quilmes , Arg</td>\n",
       "      <td>This desperation dislocation\\nSeparation conde...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2715</th>\n",
       "      <td>3900</td>\n",
       "      <td>devastated</td>\n",
       "      <td>PG Chillin!</td>\n",
       "      <td>Man Currensy really be talkin that talk... I'd...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3024</th>\n",
       "      <td>4342</td>\n",
       "      <td>dust%20storm</td>\n",
       "      <td>chicago</td>\n",
       "      <td>Going to a fest? Bring swimming goggles for th...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4068</th>\n",
       "      <td>5781</td>\n",
       "      <td>forest%20fires</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Campsite recommendations \\nToilets /shower \\nP...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4609</th>\n",
       "      <td>6552</td>\n",
       "      <td>injury</td>\n",
       "      <td>Saint Paul</td>\n",
       "      <td>My prediction for the Vikings game this Sunday...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4611</th>\n",
       "      <td>6554</td>\n",
       "      <td>injury</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Dante Exum's knee injury could stem Jazz's hop...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4622</th>\n",
       "      <td>6570</td>\n",
       "      <td>injury</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@Sport_EN Just being linked to Arsenal causes ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4713</th>\n",
       "      <td>6701</td>\n",
       "      <td>lava</td>\n",
       "      <td>Nashville, TN</td>\n",
       "      <td>Imagine a room with walls that are lava lamps.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4714</th>\n",
       "      <td>6702</td>\n",
       "      <td>lava</td>\n",
       "      <td>probably watching survivor</td>\n",
       "      <td>The sunset looked like an erupting volcano ......</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4732</th>\n",
       "      <td>6729</td>\n",
       "      <td>lava</td>\n",
       "      <td>Clayton, NC</td>\n",
       "      <td>Check out my Lava lamp dude ???? http://t.co/T...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4820</th>\n",
       "      <td>6861</td>\n",
       "      <td>mass%20murder</td>\n",
       "      <td>i'm a Citizen of the World</td>\n",
       "      <td>If abortion is murder then blowjobs are cannib...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5068</th>\n",
       "      <td>7226</td>\n",
       "      <td>natural%20disaster</td>\n",
       "      <td>on to the next adventure</td>\n",
       "      <td>Of course the one day I have to dress professi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id             keyword                    location  \\\n",
       "229    328         annihilated                         NaN   \n",
       "301    443          apocalypse                         NaN   \n",
       "356    513                army                      Studio   \n",
       "1822  2619             crashed                         NaN   \n",
       "2536  3640          desolation               Quilmes , Arg   \n",
       "2715  3900          devastated                 PG Chillin!   \n",
       "3024  4342        dust%20storm                     chicago   \n",
       "4068  5781      forest%20fires                         NaN   \n",
       "4609  6552              injury                  Saint Paul   \n",
       "4611  6554              injury                         NaN   \n",
       "4622  6570              injury                         NaN   \n",
       "4713  6701                lava               Nashville, TN   \n",
       "4714  6702                lava  probably watching survivor   \n",
       "4732  6729                lava                 Clayton, NC   \n",
       "4820  6861       mass%20murder  i'm a Citizen of the World   \n",
       "5068  7226  natural%20disaster    on to the next adventure   \n",
       "\n",
       "                                                   text  target  \n",
       "229          Ready to get annihilated for the BUCS game       1  \n",
       "301   Short Reading\\n\\nApocalypse 21:1023 \\n\\nIn the...       1  \n",
       "356   But if you build an army of 100 dogs and their...       1  \n",
       "1822  My iPod crashed..... \\n#WeLoveYouLouis \\n#MTVH...       1  \n",
       "2536  This desperation dislocation\\nSeparation conde...       1  \n",
       "2715  Man Currensy really be talkin that talk... I'd...       1  \n",
       "3024  Going to a fest? Bring swimming goggles for th...       1  \n",
       "4068  Campsite recommendations \\nToilets /shower \\nP...       1  \n",
       "4609  My prediction for the Vikings game this Sunday...       1  \n",
       "4611  Dante Exum's knee injury could stem Jazz's hop...       1  \n",
       "4622  @Sport_EN Just being linked to Arsenal causes ...       1  \n",
       "4713     Imagine a room with walls that are lava lamps.       1  \n",
       "4714  The sunset looked like an erupting volcano ......       1  \n",
       "4732  Check out my Lava lamp dude ???? http://t.co/T...       1  \n",
       "4820  If abortion is murder then blowjobs are cannib...       1  \n",
       "5068  Of course the one day I have to dress professi...       1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids_with_target_error = [328,443,513,2619,3640,3900,4342,5781,6552,6554,6570,6701,6702,6729,6861,7226]\n",
    "train_orig[train_orig['id'].isin(ids_with_target_error)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's fix these tweets records:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>229</th>\n",
       "      <td>328</td>\n",
       "      <td>annihilated</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Ready to get annihilated for the BUCS game</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301</th>\n",
       "      <td>443</td>\n",
       "      <td>apocalypse</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Short Reading\\n\\nApocalypse 21:1023 \\n\\nIn the...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>356</th>\n",
       "      <td>513</td>\n",
       "      <td>army</td>\n",
       "      <td>Studio</td>\n",
       "      <td>But if you build an army of 100 dogs and their...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1822</th>\n",
       "      <td>2619</td>\n",
       "      <td>crashed</td>\n",
       "      <td>NaN</td>\n",
       "      <td>My iPod crashed..... \\n#WeLoveYouLouis \\n#MTVH...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2536</th>\n",
       "      <td>3640</td>\n",
       "      <td>desolation</td>\n",
       "      <td>Quilmes , Arg</td>\n",
       "      <td>This desperation dislocation\\nSeparation conde...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2715</th>\n",
       "      <td>3900</td>\n",
       "      <td>devastated</td>\n",
       "      <td>PG Chillin!</td>\n",
       "      <td>Man Currensy really be talkin that talk... I'd...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3024</th>\n",
       "      <td>4342</td>\n",
       "      <td>dust%20storm</td>\n",
       "      <td>chicago</td>\n",
       "      <td>Going to a fest? Bring swimming goggles for th...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4068</th>\n",
       "      <td>5781</td>\n",
       "      <td>forest%20fires</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Campsite recommendations \\nToilets /shower \\nP...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4609</th>\n",
       "      <td>6552</td>\n",
       "      <td>injury</td>\n",
       "      <td>Saint Paul</td>\n",
       "      <td>My prediction for the Vikings game this Sunday...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4611</th>\n",
       "      <td>6554</td>\n",
       "      <td>injury</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Dante Exum's knee injury could stem Jazz's hop...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4622</th>\n",
       "      <td>6570</td>\n",
       "      <td>injury</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@Sport_EN Just being linked to Arsenal causes ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4713</th>\n",
       "      <td>6701</td>\n",
       "      <td>lava</td>\n",
       "      <td>Nashville, TN</td>\n",
       "      <td>Imagine a room with walls that are lava lamps.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4714</th>\n",
       "      <td>6702</td>\n",
       "      <td>lava</td>\n",
       "      <td>probably watching survivor</td>\n",
       "      <td>The sunset looked like an erupting volcano ......</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4732</th>\n",
       "      <td>6729</td>\n",
       "      <td>lava</td>\n",
       "      <td>Clayton, NC</td>\n",
       "      <td>Check out my Lava lamp dude ???? http://t.co/T...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4820</th>\n",
       "      <td>6861</td>\n",
       "      <td>mass%20murder</td>\n",
       "      <td>i'm a Citizen of the World</td>\n",
       "      <td>If abortion is murder then blowjobs are cannib...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5068</th>\n",
       "      <td>7226</td>\n",
       "      <td>natural%20disaster</td>\n",
       "      <td>on to the next adventure</td>\n",
       "      <td>Of course the one day I have to dress professi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id             keyword                    location  \\\n",
       "229    328         annihilated                         NaN   \n",
       "301    443          apocalypse                         NaN   \n",
       "356    513                army                      Studio   \n",
       "1822  2619             crashed                         NaN   \n",
       "2536  3640          desolation               Quilmes , Arg   \n",
       "2715  3900          devastated                 PG Chillin!   \n",
       "3024  4342        dust%20storm                     chicago   \n",
       "4068  5781      forest%20fires                         NaN   \n",
       "4609  6552              injury                  Saint Paul   \n",
       "4611  6554              injury                         NaN   \n",
       "4622  6570              injury                         NaN   \n",
       "4713  6701                lava               Nashville, TN   \n",
       "4714  6702                lava  probably watching survivor   \n",
       "4732  6729                lava                 Clayton, NC   \n",
       "4820  6861       mass%20murder  i'm a Citizen of the World   \n",
       "5068  7226  natural%20disaster    on to the next adventure   \n",
       "\n",
       "                                                   text  target  \n",
       "229          Ready to get annihilated for the BUCS game       0  \n",
       "301   Short Reading\\n\\nApocalypse 21:1023 \\n\\nIn the...       0  \n",
       "356   But if you build an army of 100 dogs and their...       0  \n",
       "1822  My iPod crashed..... \\n#WeLoveYouLouis \\n#MTVH...       0  \n",
       "2536  This desperation dislocation\\nSeparation conde...       0  \n",
       "2715  Man Currensy really be talkin that talk... I'd...       0  \n",
       "3024  Going to a fest? Bring swimming goggles for th...       0  \n",
       "4068  Campsite recommendations \\nToilets /shower \\nP...       0  \n",
       "4609  My prediction for the Vikings game this Sunday...       0  \n",
       "4611  Dante Exum's knee injury could stem Jazz's hop...       0  \n",
       "4622  @Sport_EN Just being linked to Arsenal causes ...       0  \n",
       "4713     Imagine a room with walls that are lava lamps.       0  \n",
       "4714  The sunset looked like an erupting volcano ......       0  \n",
       "4732  Check out my Lava lamp dude ???? http://t.co/T...       0  \n",
       "4820  If abortion is murder then blowjobs are cannib...       0  \n",
       "5068  Of course the one day I have to dress professi...       0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_orig.at[train_orig['id'].isin(ids_with_target_error),'target'] = 0\n",
    "train_orig[train_orig['id'].isin(ids_with_target_error)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After doing this, luckily (or not) score could be slightly higher. But for sure our model is slightly better! Imho this could be a hint about: how to improve your score/model. There is much more work about this training data as far as I can see :) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### My second shot was to take tweets from another source. Luckily there is a kaggle dataset containing 1.6m tweets that can be found here: https://www.kaggle.com/kazanova/sentiment140\n",
    "\n",
    "Although some of them are tweeted around 10 years ago, I choose randomly **1000** of them, and only **16 **were classified as **real disaster**. However, after reading these tweets I find that only two of those are about disaster! After corrections, our additional training data looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2237307600</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>it is raining again</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2001169708</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>We had baseball game on Sunday we lose 2-0 :/ ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1932411345</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All my prayers goes out to Rodney Rodgers and ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2050120420</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>good news!! i got a bike!! bad news... my car ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1957522146</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Sakasama no Chou - Upside down butterfly.....?!</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1993962054</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Weather is not good for launching a hotair bal...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2061831345</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Hey I just came back from Darien lake it was f...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2228776046</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Flights delayed out of NY. Rerouted flights to...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2183599231</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@Goodness500 that river in Indo sure looks nas...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1993920064</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>first time in 2 weeks that i actually studied ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1985612060</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I just watched a woman lose her husband on TV....</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1984650281</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2 and 1/2 more hours. thank goodness!</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2000690291</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@tyronevh 2 cold fronts today</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2324653382</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>no classes for 2 days because of typhoon Feria</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2065544107</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>@tommcfly you really don't want to be back at ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2002832172</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>is watching &amp;quot;Crush night&amp;quot;. Which tak...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            id  keyword  location  \\\n",
       "0   2237307600      NaN       NaN   \n",
       "1   2001169708      NaN       NaN   \n",
       "2   1932411345      NaN       NaN   \n",
       "3   2050120420      NaN       NaN   \n",
       "4   1957522146      NaN       NaN   \n",
       "5   1993962054      NaN       NaN   \n",
       "6   2061831345      NaN       NaN   \n",
       "7   2228776046      NaN       NaN   \n",
       "8   2183599231      NaN       NaN   \n",
       "9   1993920064      NaN       NaN   \n",
       "10  1985612060      NaN       NaN   \n",
       "11  1984650281      NaN       NaN   \n",
       "12  2000690291      NaN       NaN   \n",
       "13  2324653382      NaN       NaN   \n",
       "14  2065544107      NaN       NaN   \n",
       "15  2002832172      NaN       NaN   \n",
       "\n",
       "                                                 text  target  \n",
       "0                                it is raining again        0  \n",
       "1   We had baseball game on Sunday we lose 2-0 :/ ...       0  \n",
       "2   All my prayers goes out to Rodney Rodgers and ...       1  \n",
       "3   good news!! i got a bike!! bad news... my car ...       0  \n",
       "4    Sakasama no Chou - Upside down butterfly.....?!        0  \n",
       "5   Weather is not good for launching a hotair bal...       0  \n",
       "6   Hey I just came back from Darien lake it was f...       0  \n",
       "7   Flights delayed out of NY. Rerouted flights to...       0  \n",
       "8   @Goodness500 that river in Indo sure looks nas...       0  \n",
       "9   first time in 2 weeks that i actually studied ...       0  \n",
       "10  I just watched a woman lose her husband on TV....       0  \n",
       "11             2 and 1/2 more hours. thank goodness!        0  \n",
       "12                     @tyronevh 2 cold fronts today        0  \n",
       "13    no classes for 2 days because of typhoon Feria        1  \n",
       "14  @tommcfly you really don't want to be back at ...       0  \n",
       "15  is watching &quot;Crush night&quot;. Which tak...       0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_add = pd.read_csv(\"/kaggle/input/real-or-not-nlp-with-disaster-tweets-addings/train_add.csv\")\n",
    "train_add"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's an important and hard lesson for our classifier. Let's append it to the original training data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7629, 5)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = train_orig.append(train_add)\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Good luck!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# About this kernel\n",
    "\n",
    "I've seen a lot of people pooling the output of BERT, then add some Dense layers. I also saw various learning rates for fine-tuning. In this kernel, I wanted to try some ideas that were used in the original paper that did not appear in many public kernel. Here are some examples:\n",
    "* *No pooling, directly use the CLS embedding*. The original paper uses the output embedding for the `[CLS]` token when it is finetuning for classification tasks, such as sentiment analysis. Since the `[CLS]` token is the first token in our sequence, we simply take the first slice of the 2nd dimension from our tensor of shape `(batch_size, max_len, hidden_dim)`, which result in a tensor of shape `(batch_size, hidden_dim)`.\n",
    "* *No Dense layer*. Simply add a sigmoid output directly to the last layer of BERT, rather than experimenting with different intermediate layers.\n",
    "* *Fixed learning rate, batch size, epochs, optimizer*. As specified by the paper, the optimizer used is Adam, with a learning rate between 2e-5 and 5e-5. Furthermore, they train the model for 3 epochs with a batch size of 32. I wanted to see how well it would perform with those default values.\n",
    "\n",
    "I also wanted to share this kernel as a **concise, reusable, and functional example of how to build a workflow around the TF2 version of BERT**. Indeed, it takes less than **50 lines of code to write a string-to-tokens preprocessing function and model builder**.\n",
    "\n",
    "## References\n",
    "\n",
    "* Source for `bert_encode` function: https://www.kaggle.com/user123454321/bert-starter-inference\n",
    "* All pre-trained BERT models from Tensorflow Hub: https://tfhub.dev/s?q=bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use the official tokenization script created by the Google team\n",
    "!wget --quiet https://raw.githubusercontent.com/tensorflow/models/master/official/nlp/bert/tokenization.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "import tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_encode(texts, tokenizer, max_len=512):\n",
    "    all_tokens = []\n",
    "    all_masks = []\n",
    "    all_segments = []\n",
    "    \n",
    "    for text in texts:\n",
    "        text = tokenizer.tokenize(text)\n",
    "            \n",
    "        text = text[:max_len-2]\n",
    "        input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\n",
    "        pad_len = max_len - len(input_sequence)\n",
    "        \n",
    "        tokens = tokenizer.convert_tokens_to_ids(input_sequence)\n",
    "        tokens += [0] * pad_len\n",
    "        pad_masks = [1] * len(input_sequence) + [0] * pad_len\n",
    "        segment_ids = [0] * max_len\n",
    "        \n",
    "        all_tokens.append(tokens)\n",
    "        all_masks.append(pad_masks)\n",
    "        all_segments.append(segment_ids)\n",
    "    \n",
    "    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "def build_model(bert_layer, max_len=512):\n",
    "    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n",
    "    input_mask = Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n",
    "    segment_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"segment_ids\")\n",
    "\n",
    "    _, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n",
    "    clf_output = sequence_output[:, 0, :]\n",
    "    out = Dense(1, activation='sigmoid')(clf_output)\n",
    "    \n",
    "    model = Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=out)\n",
    "    model.compile(Adam(lr=1e-5), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and Preprocess\n",
    "\n",
    "- Load BERT from the Tensorflow Hub\n",
    "- Load CSV files containing training data\n",
    "- Load tokenizer from the bert layer\n",
    "- Encode the text into tokens, masks, and segment flags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 25s, sys: 8.73 s, total: 1min 34s\n",
      "Wall time: 1min 37s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "module_url = \"https://tfhub.dev/tensorflow/bert_en_uncased_L-24_H-1024_A-16/1\"\n",
    "bert_layer = hub.KerasLayer(module_url, trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"/kaggle/input/nlp-getting-started/train.csv\")\n",
    "test = pd.read_csv(\"/kaggle/input/nlp-getting-started/test.csv\")\n",
    "submission = pd.read_csv(\"/kaggle/input/nlp-getting-started/sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
    "do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
    "tokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input = bert_encode(train.text.values, tokenizer, max_len=160)\n",
    "test_input = bert_encode(test.text.values, tokenizer, max_len=160)\n",
    "train_labels = train.target.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model: Build, Train, Predict, Submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_word_ids (InputLayer)     [(None, 160)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_mask (InputLayer)         [(None, 160)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "segment_ids (InputLayer)        [(None, 160)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "keras_layer (KerasLayer)        [(None, 1024), (None 335141889   input_word_ids[0][0]             \n",
      "                                                                 input_mask[0][0]                 \n",
      "                                                                 segment_ids[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice (Tens [(None, 1024)]       0           keras_layer[0][1]                \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 1)            1025        tf_op_layer_strided_slice[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 335,142,914\n",
      "Trainable params: 335,142,913\n",
      "Non-trainable params: 1\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_model(bert_layer, max_len=160)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6090 samples, validate on 1523 samples\n",
      "Epoch 1/3\n",
      "6090/6090 [==============================] - 427s 70ms/sample - loss: 0.4220 - accuracy: 0.8167 - val_loss: 0.3659 - val_accuracy: 0.8496\n",
      "Epoch 2/3\n",
      "6090/6090 [==============================] - 354s 58ms/sample - loss: 0.2735 - accuracy: 0.8895 - val_loss: 0.3793 - val_accuracy: 0.8385\n",
      "Epoch 3/3\n",
      "6090/6090 [==============================] - 354s 58ms/sample - loss: 0.1416 - accuracy: 0.9453 - val_loss: 0.5192 - val_accuracy: 0.8227\n"
     ]
    }
   ],
   "source": [
    "checkpoint = ModelCheckpoint('model.h5', monitor='val_loss', save_best_only=True)\n",
    "\n",
    "train_history = model.fit(\n",
    "    train_input, train_labels,\n",
    "    validation_split=0.2,\n",
    "    epochs=3,\n",
    "    callbacks=[checkpoint],\n",
    "    batch_size=16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('model.h5')\n",
    "test_pred = model.predict(test_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission['target'] = test_pred.round().astype(int)\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
