{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the official tokenization script created by the Google team. Ensure GPU & Internet is turned on for this kernel\n",
    "!wget --quiet https://raw.githubusercontent.com/tensorflow/models/master/official/nlp/bert/tokenization.py\n",
    "    \n",
    "#!wget https://raw.githubusercontent.com/tensorflow/models/master/official/nlp/bert/tokenization.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import all the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Import libraries complete...\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "import re\n",
    "import math\n",
    "import gc\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import tensorflow_hub as hub\n",
    "    \n",
    "import tokenization\n",
    "print(\"Import libraries complete...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Define Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defined support functions...\n"
     ]
    }
   ],
   "source": [
    "def memory_usage_mb(df, *args, **kwargs):\n",
    "    \"\"\"Dataframe memory usage in MB. \"\"\"\n",
    "    return df.memory_usage(*args, **kwargs).sum() / 1024**2\n",
    "\n",
    "\n",
    "\n",
    "def reduce_memory_usage(df, deep=True, verbose=True):\n",
    "    # All types that we want to change for \"lighter\" ones.\n",
    "    # int8 and float16 are not include because we cannot reduce\n",
    "    # those data types.\n",
    "    # float32 is not include because float16 has too low precision.\n",
    "    numeric2reduce = [\"int16\", \"int32\", \"int64\", \"float64\"]\n",
    "    start_mem = 0\n",
    "    if verbose:\n",
    "        start_mem = memory_usage_mb(df, deep=deep)\n",
    "\n",
    "    for col, col_type in df.dtypes.iteritems():\n",
    "        best_type = None\n",
    "        if col_type in numeric2reduce:\n",
    "            downcast = \"integer\" if \"int\" in str(col_type) else \"float\"\n",
    "            df[col] = pd.to_numeric(df[col], downcast=downcast)\n",
    "            best_type = df[col].dtype.name\n",
    "        # Log the conversion performed.\n",
    "        if verbose and best_type is not None and best_type != str(col_type):\n",
    "            print(f\"Column '{col}' converted from {col_type} to {best_type}\")\n",
    "    \n",
    "    if verbose:\n",
    "        end_mem = memory_usage_mb(df, deep=deep)\n",
    "        diff_mem = start_mem - end_mem\n",
    "        percent_mem = 100 * diff_mem / start_mem\n",
    "        print(f\"Memory usage decreased from\"\n",
    "              f\" {start_mem:.2f}MB to {end_mem:.2f}MB\"\n",
    "              f\" ({diff_mem:.2f}MB, {percent_mem:.2f}% reduction)\")\n",
    "        \n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def bert_encode(texts, tokenizer, max_len=512):\n",
    "    all_tokens = []\n",
    "    all_masks = []\n",
    "    all_segments = []\n",
    "    \n",
    "    for text in texts:\n",
    "        text = tokenizer.tokenize(text)\n",
    "            \n",
    "        text = text[:max_len-2]\n",
    "        input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\n",
    "        pad_len = max_len - len(input_sequence)\n",
    "        \n",
    "        tokens = tokenizer.convert_tokens_to_ids(input_sequence)\n",
    "        tokens += [0] * pad_len\n",
    "        pad_masks = [1] * len(input_sequence) + [0] * pad_len\n",
    "        segment_ids = [0] * max_len\n",
    "        \n",
    "        all_tokens.append(tokens)\n",
    "        all_masks.append(pad_masks)\n",
    "        all_segments.append(segment_ids)\n",
    "    \n",
    "    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)\n",
    "\n",
    "\n",
    "\n",
    "def build_model(bert_layer, max_len=512):\n",
    "    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n",
    "    input_mask = Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n",
    "    segment_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"segment_ids\")\n",
    "\n",
    "    _, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n",
    "    clf_output = sequence_output[:, 0, :]\n",
    "    out = Dense(1, activation='sigmoid')(clf_output)\n",
    "    \n",
    "    model = Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=out)\n",
    "    model.compile(Adam(lr=2e-6), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "print(\"Defined support functions...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train :  (7613, 5)\n",
      "Test  :  (3263, 4)\n",
      "[1 0]\n",
      "(10876, 5)\n",
      "Data load complete...\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv(\"../input/nlp-getting-started/train.csv\")\n",
    "test_df  = pd.read_csv(\"../input/nlp-getting-started/test.csv\")\n",
    "sub_df   = pd.read_csv(\"../input/nlp-getting-started/sample_submission.csv\")\n",
    "\n",
    "\n",
    "print(\"Train : \",train_df.shape)         # -- (7613, 5)\n",
    "print(\"Test  : \", test_df.shape)         # -- (3263, 5)\n",
    "print(train_df.target.unique())\n",
    "\n",
    "test_df['target']=-1\n",
    "\n",
    "# We create a full dataset with train and test values\n",
    "\n",
    "df_full = pd.concat([train_df, test_df], sort=True)\n",
    "print(df_full.shape)                     # -- (10876, 5)\n",
    "print(\"Data load complete...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id       :  0.0 % Missing values\n",
      "keyword  :  0.8 % Missing values ;  87 Missing records\n",
      "location :  33.45 % Missing values ;  3638 Missing records\n",
      "text     :  0.0 % Missing values\n"
     ]
    }
   ],
   "source": [
    "# Determine percentage of missing values\n",
    "\n",
    "print('id       : ' , round(len(df_full.loc[df_full['id'].isna()])/len(df_full) *100, 2), \"% Missing values\")\n",
    "print('keyword  : ' , round(len(df_full.loc[df_full['keyword'].isna()])/len(df_full) *100, 2), \"% Missing values ; \", round(len(df_full.loc[df_full['keyword'].isna()])), \"Missing records\")\n",
    "print('location : ' , round(len(df_full.loc[df_full['location'].isna()])/len(df_full) *100, 2), \"% Missing values ; \", round(len(df_full.loc[df_full['location'].isna()])), \"Missing records\")\n",
    "print('text     : ' , round(len(df_full.loc[df_full['text'].isna()])/len(df_full) *100, 2), \"% Missing values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted data to lowercase...\n"
     ]
    }
   ],
   "source": [
    "# Convert to lowercase\n",
    "df_full['keyword'] = df_full['keyword'].str.lower()\n",
    "df_full['location'] = df_full['location'].str.lower()\n",
    "df_full['text'] = df_full['text'].str.lower()\n",
    "\n",
    "print(\"Converted data to lowercase...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic data cleanup complete...\n"
     ]
    }
   ],
   "source": [
    "# Basic Data Cleanup \n",
    "\n",
    "df_full['keyword'] = df_full['keyword'].str.replace('%20',' ')\n",
    "df_full['location'] = df_full['location'].str.replace('?','')\n",
    "df_full['location'] = df_full['location'].str.replace('(','')\n",
    "df_full['location'] = df_full['location'].str.replace(')','')\n",
    "df_full['location'] = df_full['location'].str.replace('\\x89û¢','')\n",
    "\n",
    "#df_full.loc[df_full['loc2'].notnull()]\n",
    "\n",
    "#Replace irrelevant words in location with 'unknown' \n",
    "trash_list = ['your','all around','world wide','trash', 'void', 'they', 'them','nowhere', ' bae', 'webcam'\n",
    "                  'every', 'eatin','there','imagine','@','who','you', 'universe','club', 'peach','surv'] \n",
    "\n",
    "pattern = '|'.join(trash_list)     # joining list for comparision\n",
    "df_full['loc1']=df_full['location'].str.contains(pattern)\n",
    "df_full[df_full['loc1']==True]\n",
    "df_full.loc[df_full['loc1']==True,'location'] = 'unknown'\n",
    "\n",
    "del df_full['loc1']\n",
    "\n",
    "print(\"Basic data cleanup complete...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filled missing values to 'unknown'...\n"
     ]
    }
   ],
   "source": [
    "# Put 'unknown' for missing values\n",
    "df_full.fillna(value='unknown', inplace=True)\n",
    "\n",
    "df_full.loc[df_full['location'].str.contains('\\*'),'location'] ='unknown'\n",
    "df_full.loc[df_full['location'].str.contains('don\\'t'),'location'] ='unknown'\n",
    "\n",
    "print(\"Filled missing values to 'unknown'...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We check the number of unique values in those keyword and location fields:\n",
    "* keyword  :  222 records\n",
    "* location : 4522 records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "222 unique keyword values\n",
      "4198 unique location values\n"
     ]
    }
   ],
   "source": [
    "keyword_list = df_full.keyword.unique()             # -- 222 records\n",
    "location_list = df_full.location.unique()           # -- 4198 records\n",
    "\n",
    "print(len(keyword_list),\"unique keyword values\")             # -- 222 unique keyword values\n",
    "print(len(location_list),\"unique location values\")           # -- 4198 unique location values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reduce memory usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7613, 5)\n",
      "(3263, 5)\n",
      "(10876, 5)\n",
      "Column 'id' converted from int64 to int16\n",
      "Column 'target' converted from int64 to int8\n",
      "Memory usage decreased from 2.39MB to 2.30MB (0.09MB, 3.94% reduction)\n",
      "Column 'id' converted from int64 to int16\n",
      "Memory usage decreased from 1.01MB to 0.99MB (0.02MB, 1.86% reduction)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = df_full[df_full['target']!=-1]\n",
    "test_df = df_full[df_full['target']==-1]\n",
    "\n",
    "print(train_df.shape)\n",
    "print(test_df.shape)\n",
    "print(df_full.shape)\n",
    "\n",
    "del df_full\n",
    "del test_df['target']\n",
    "reduce_memory_usage(train_df)\n",
    "reduce_memory_usage(test_df)\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load and Preprocess**\n",
    "* Load BERT from the Tensorflow Hub\n",
    "* Load tokenizer from the bert layer\n",
    "* Encode the text into tokens, masks, and segment flags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 32s, sys: 9.72 s, total: 1min 41s\n",
      "Wall time: 1min 45s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "module_url = \"https://tfhub.dev/tensorflow/bert_en_uncased_L-24_H-1024_A-16/1\"\n",
    "bert_layer = hub.KerasLayer(module_url, trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
    "do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
    "tokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input = bert_encode(train_df.text.values, tokenizer, max_len=160)\n",
    "test_input = bert_encode(test_df.text.values, tokenizer, max_len=160)\n",
    "train_labels = train_df.target.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_word_ids (InputLayer)     [(None, 160)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_mask (InputLayer)         [(None, 160)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "segment_ids (InputLayer)        [(None, 160)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "keras_layer (KerasLayer)        [(None, 1024), (None 335141889   input_word_ids[0][0]             \n",
      "                                                                 input_mask[0][0]                 \n",
      "                                                                 segment_ids[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice (Tens [(None, 1024)]       0           keras_layer[0][1]                \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 1)            1025        tf_op_layer_strided_slice[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 335,142,914\n",
      "Trainable params: 335,142,913\n",
      "Non-trainable params: 1\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_model(bert_layer, max_len=160)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6090 samples, validate on 1523 samples\n",
      "Epoch 1/3\n",
      "6090/6090 [==============================] - 401s 66ms/sample - loss: 0.4671 - accuracy: 0.7893 - val_loss: 0.3943 - val_accuracy: 0.8247\n",
      "Epoch 2/3\n",
      "6090/6090 [==============================] - 354s 58ms/sample - loss: 0.3293 - accuracy: 0.8637 - val_loss: 0.4019 - val_accuracy: 0.8332\n",
      "Epoch 3/3\n",
      "6090/6090 [==============================] - 354s 58ms/sample - loss: 0.2377 - accuracy: 0.9080 - val_loss: 0.4236 - val_accuracy: 0.8280\n"
     ]
    }
   ],
   "source": [
    "train_history = model.fit(\n",
    "    train_input, train_labels,\n",
    "    validation_split=0.2,\n",
    "    epochs=3,\n",
    "    batch_size=16\n",
    ")\n",
    "\n",
    "model.save('bert_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = model.predict(test_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df['target'] = test_pred.round().astype(int)\n",
    "sub_df.to_csv('submission_bert.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
